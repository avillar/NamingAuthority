
Open GIS Consortium
35 Main Street, Suite 5
Wayland, MA 01778

Telephone: +1-508-655-5858
Facsimile: +1-508-655-2237

Editor:
Telephone: +1-703-830-6516
Facsimile: +1-703-830-7096

ckottman@opengis.org

The OpenGIS™ Abstract Specification
Topic 9: Quality

Version 4

OpenGIS™ Project Document Number 99-109r1.doc



The OpenGIS™ Abstract Specification

Copyright © 1999, Open GIS Consortium, Inc.

NOTICE

The information contained in this document is subject to change without notice.

The material in this document details an Open GIS Consortium (OGC) specification in accordance with the license and
notice set forth on this page. This document does not represent a commitment to implement any portion of this
specification in any companies’ products.

While the information in this publication is beleived to be accurate, the Open GIS Consortium makes no warranty of
any kind with regard to this material including but not limited to the implied warranties of merchantability and fitness
for a particular purpose. The Open GIS Consortium shall not be liable for errors contained herein or for incidental or
consequential damages in connection with the furnishing, performance or use of this material. The information
contained in this document is subject to change without notice.

The Open GIS Consortium is and shall at all times be the sole entity that may authorize developers, suppliers and
sellers of computer software to use certification marks, trademarks, or other special designations to indicate compliance
with these materials.

This document contains information which is protected by copyright. All Rights Reserved. Except as otherwise
provided herein, no part of this work may be reproduced or used in any form or by any means (graphic, electronic, or
mechanical including photocopying, recording, taping, or information storage and retrieval systems) without the
permission of the copyright owner. All copies of this document must include the copyright and other information
contained on this page.

The copyright owner grants member companies of the OGC permission to make a limited number of copies of this
document (up to fifty copies) for their internal use as a part of the OGC Technology Development process.



The OpenGIS™ Abstract Specification Page i

Volume 9: Quality (99-109r1.doc)

Revision History

Date Description
2 March 1998 98-109r2 “Revision 3”
15 January 1999 Update with revisions from 98-026: convert the object diagrams from OMT to UML, correct

some wording and change terms to be consistent with ISO metadata and Topic 11; update
copyright for 1999; use revised document template

30 March 1999 Update Section 1 with new boilerplate; move former Section 2.1 to Section 1.2.



The OpenGIS™ Abstract Specification Page ii

Volume 9: Quality (99-109r1.doc)

This page is intentionally left blank.



The OpenGIS™ Abstract Specification Page iii

Volume 9: Quality (99-109r1.doc)

Table of Contents

1. Introduction.............................................................................................. 1
1.1. The Abstract Specification ..................................................................................1
1.2. Introduction to Quality ........................................................................................1
1.3. References for Section 1.......................................................................................2

2. The Essential Model for Quality ............................................................ 3
2.1. Absolute Accuracy................................................................................................3
2.2. Relative Accuracy.................................................................................................3
2.3. Value Accuracy.....................................................................................................3
2.4. Vertical Linear Error...........................................................................................3
2.5. Horizontal Circular Error ...................................................................................3
2.6. 3-D Spherical Error..............................................................................................4
2.7. Covariance Matrix ...............................................................................................4
2.8. Confidence Probability ........................................................................................4
2.9. Normal Error Distribution ..................................................................................5
2.10. Status of the Abstract Specification for Quality ..............................................5
2.11. References for Section 2 .....................................................................................5

3. Abstract Model for Quality..................................................................... 6
3.1. Accuracy and Other Metadata............................................................................6

3.1.1. Feature....................................................................................................................................7
3.1.2. Feature Collection ..................................................................................................................7
3.1.3. Metadata Set ...........................................................................................................................8
3.1.4. Metadata Entity ......................................................................................................................8

3.2. Position Accuracy Metadata Entities .................................................................8
3.2.1. Absolute Position Accuracy....................................................................................................8
3.2.2. Relative Position Accuracy.....................................................................................................8

3.3. Image Position Accuracy Metadata Entities......................................................9
3.3.1. Monoscopic Image Absolute Accuracy................................................................................10
3.3.2. Monoscopic Image Relative Accuracy .................................................................................10
3.3.3. Stereoscopic Images Absolute Accuracy..............................................................................11
3.3.4. Stereoscopic Images Relative Accuracy...............................................................................12

3.4. Property Accuracy Metadata Entities ..............................................................14
3.4.1. Metric Property Accuracy ....................................................................................................14
3.4.2. Non-metric Property Accuracy.............................................................................................14
3.4.3. Intersection Threshold .........................................................................................................14

3.5. Discussion............................................................................................................15
3.6. Interpretations ....................................................................................................15
3.7. Assumptions ........................................................................................................15
3.8. Alternatives .........................................................................................................16
3.9. Error Estimation Methods ................................................................................17
3.10. Possible Levels of Position Accuracy...............................................................17
3.11. Possible Levels of Property Accuracy .............................................................18



The OpenGIS™ Abstract Specification Page iv

Volume 9: Quality (99-109r1.doc)

3.12. CE and LE Computations................................................................................18
3.13. References for Section 3 ...................................................................................19

4. Future Work........................................................................................... 20

5. Appendix A. Well Known Structures .................................................. 21



The OpenGIS™ Abstract Specification Page 1

Volume 9: Quality (99-109r1.doc)

1. Introduction

1.1. The Abstract Specification
The purpose of the Abstract Specification is to create and document a conceptual model sufficient
enough to allow for the creation of Implementation Specifications. The Abstract Specification
consists of two models derived from the Syntropy object analysis and design methodology [1].

The first and simpler model is called the Essential Model and its purpose is to establish the
conceptual linkage of the software or system design to the real world. The Essential Model is a
description of how the world works (or should work).

The second model, the meat of the Abstract Specification, is the Abstract Model that defines the
eventual software system in an implementation neutral manner. The Abstract Model is a description
of how software should work. The Abstract Model represents a compromise between the paradigms
of the intended target implementation environments.

The Abstract Specification is organized into separate topic volumes in order to manage the
complexity of the subject matter and to assist parallel development of work items by different
Working Groups of the OGC Technical Committee. The topics are, in reality, dependent upon one
another each one begging to be written first. Each topic must be read in the context of the entire
Abstract Specification.

The topic volumes are not all written at the same level of detail.  Some are mature, and are the basis
for Requests For Proposal (RFP). Others are immature, and require additional specification before
RFPs can be issued. The level of maturity of a topic reflects the level of understanding and
discussion occurring within the Technical Committee. Refer to the OGC Technical Committee
Policies and Procedures [2] and Technology Development Process [3] documents for more
information on the OGC OpenGIS™ standards development process.

Refer to Topic Volume 0: Abstract Specification Overview [4] for an introduction to all of the topic
volumes comprising the Abstract Specification and for editorial guidance, rules and etiquette for
authors (and readers) of OGC specifications.

1.2. Introduction to Quality
This document specifies extensions to the OpenGIS™ Abstract Specification to support recording
the quality of "Feature," "Geometry," and “Coverage” objects. This abstract specification is
submitted in response to recurring requests in OGC meetings for someone to detail how quality and
accuracy should be handled. The specified extensions support recording the position accuracy of
Feature objects, and recording the accuracy and quality of other Feature properties. Such feature
quality and accuracy data is one category of metadata, and this specification includes partial
capabilities for recording other metadata associated with each Feature.

In the Abstract Specification, a Feature object can have multiple properties, some of which have
numerical values. One property of a Feature is often a Geometry, which defines the location of the
Feature in 2 or 3 dimensional space. For a simple Geometry property, locations are specified in
OpenGIS Well Known Structures (WKS) by the positions of one or more points, relative to a
specified spatial reference system. A point can be a Point Geometry, or can be a component of a
more complex Geometry. Each point has numerical values giving its position coordinates in 2 or 3
dimensional space.

A subclass of the Feature abstract class is the Coverage class. The Coverage class has a Coverage
Generator property that is a function often called G, instead of having a Geometry property with a
value that is an OpenGIS WKS. A subclass of the Coverage class is the Image class. For an image,
the Coverage Generator function G relates positions in the Project World (usually in ground
coordinates) to the corresponding positions in the image (in image coordinates). The ground
coordinates used in G are in a specified spatial reference system.

The numerical values of (ground) point coordinates always have limited accuracies. If the accuracy
were truly unknown, the numerical value would have little practical value. If the accuracy is
known, that accuracy should be communicated from the data producer to the data user, so that the
data user can properly interpret and use the numerical value. Recording of position accuracy
information is therefore required by the OpenGIS™ Abstract Specification.



The OpenGIS™ Abstract Specification Page 2

Volume 9: Quality (99-109r1.doc)

This proposal takes the “truth in labeling” approach. That is, the GIS data producer is expected to
provide numerical accuracy data with GIS data provided. The data user is then expected to use that
accuracy data to determine how to use this GIS data. A prospective user should use the accuracy
data to check if available GIS data is accurate enough to support the intended use.  (To the
maximum practical degree, this checking should be done automatically by user software, instead of
done manually by a human user.)  This “truth in labeling” approach is employed in the Draft
Geospatial Positioning Accuracy Standards, from the United States Federal Geospatial Data
Committee (FGDC).

The “truth in labeling” approach is used instead of (or in addition to) a “suitable use labeling”
approach. In a “suitable use labeling” approach, GIS data would be labeled with the data uses
considered suitable. For example, GIS data might be labeled as suitable for printing a 1:24,000
scale paper map (that meets the old United States National Map Accuracy Standard). The “truth in
labeling” approach is more suitable when there are many and increasing potential uses of any GIS
data.

Many alternative forms of position accuracy could be recorded, ranging from very simple to very
complex. This proposal specifies one way in which to record most of the position accuracy data
currently used for images by the National Imagery and Mapping Agency (NIMA). This NIMA
image accuracy data is fairly extensive and complex, and may not be needed by many users of a
GIS. Therefore, this proposal allows recording of only a subset of the defined accuracy data.

The following sections of this topic volume define and discuss various position accuracy terms and
concepts used later in this proposal.

1.3. References for Section 1
[1] Cook, Steve, and John Daniels, Designing Objects Systems: Object-Oriented Modeling with

Syntropy, Prentice Hall, New York, 1994, xx + 389 pp.

[2] Open GIS Consortium, 1997. OGC Technical Committee Policies and Procedures, Wayland,
Massachusetts. Available via the WWW as <http://www.opengis.org/techno/development.htm>.

[3] Open GIS Consortium, 1997. The OGC Technical Committee Technology Development Process,
Wayland, Massachusetts. Available via the WWW as
<http://www.opengis.org/techno/development.htm>.

[4] Open GIS Consortium, 1999.  Topic 0, Abstract Specification Overview, Wayland, Massachusetts.
Available via the WWW as <http://www.opengis.org/techno/specs.htm>.



The OpenGIS™ Abstract Specification Page 3

Volume 9: Quality (99-109r1.doc)

2. The Essential Model for Quality

2.1. Absolute Accuracy
Absolute accuracy is the error estimate for a single point, relative to the specified spatial reference
system (for example, WGS-84). This accuracy includes errors from all known and expected
sources. The error estimate from a particular error source is usually called an error component.

2.2. Relative Accuracy
Relative accuracy is the error estimate for the distance between two points, or the accuracy of one
point with respect to the other point. This accuracy includes errors from all known and expected
sources. If the errors at two points are not correlated, the relative error is the Root Mean Square
(RMS) of the individual point absolute errors. If the absolute errors are essentially the same at two
points, the relative error is thus about 1.4 times the individual point absolute errors.

However, if the errors at two points are highly correlated, the relative error can be significantly less
than the individual point absolute errors. For example, some error components can be exactly or
approximately the same at two points. The relative accuracy excludes those error components that
are the same at the two points. The relative accuracy includes those error components that are
statistically independent at the two points, including such error components for each point.

Relative accuracy usually varies with the distance between the two points, being smaller for shorter
distances. When the nominal distance between two points is not fixed, the accuracy variation with
distance is often reflected by giving the relative accuracy estimate for each of several distance bins.
Each distance bin specifies the minimum and maximum vector distances between two points, for
which the stated relative accuracy applies. Alternately, the accuracy variation with distance might
be specified by giving the relative accuracy as a mathematical function of the distance between
points.

2.3. Value Accuracy
The accuracy of a numerical value is usually defined by giving one or more statistical measures of
the expected error in the value, or in a group of similar values. If a specific, non-statistical measure
of the error were known, the value should be changed to eliminate that error. Similarly, if an
(arithmetic) average error were present and known, the value(s) should be changed to eliminate the
average error.

In general, the possible error values have a probability distribution, which defines the probability or
probability density of each error value. However, a complete probability distribution is usually
much more detailed than is needed, so a simple statistical error summary is usually used. This
summary often defines an error magnitude and the probability that the actual error is less than this
magnitude. For example, we may say that a value has 0.9 probability of having an error
(magnitude) less than 7 meters.

2.4. Vertical Linear Error
When a vertical coordinate, such as height or elevation, is defined for a ground point, the vertical
coordinate accuracy should be specified. Vertical accuracy is often specified using a “Linear Error”
or “LE”. In a Linear Error, we record that the value has a specified probability of having an error
magnitude less than a specified distance. For example, we may say that the height value has 0.9
probability of having an error magnitude less than 7 meters. Vertical Linear Error can be used to
record either absolute or relative accuracy.

2.5. Horizontal Circular Error
Two horizontal coordinates, such as Latitude and Longitude, are usually defined for a ground point,
and the horizontal position accuracy should be specified. The accuracy of each horizontal
coordinate could be separately specified, perhaps using a “Linear Error” or “LE”. However, the
composite horizontal accuracy is often specified using a “Circular Error” or “CE”. In a Circular
Error, we record that the horizontal position has a specified probability of having a vector error
magnitude less than a specified distance. For example, we may say that the horizontal position has
0.5 probability of having a vector error magnitude less than 11 meters. Use of a Circular Error is



The OpenGIS™ Abstract Specification Page 4

Volume 9: Quality (99-109r1.doc)

appropriate when the accuracies are similar in the two horizontal coordinates. Horizontal Circular
Error can be used to record either absolute or relative accuracy.

2.6. 3-D Spherical Error
The accuracy of 3-D ground coordinates are often specified using a Horizontal Circular Error plus a
Vertical Linear Error. Alternately, the accuracy of 3-D coordinates might be specified using a
“Spherical Error.” A spherical error records that the 3-D position has a specified probability of
having a vector error magnitude less than a specified distance.

Use of a Spherical Error would be appropriate if the accuracies are similar in all three ground
coordinates. However, the vertical coordinate error is often either undefined or significantly
different from the horizontal coordinates linear errors. If wanted, the approximate Spherical Error
can be computed from the Horizontal Circular Error plus Vertical Linear Error, if uncorrelated
errors and a normal distribution are assumed. 3-D Spherical Error could be used to record either
absolute or relative accuracy.

2.7. Covariance Matrix
When there are 2 or 3 ground coordinates, more detailed accuracy information can be recorded
using a covariance matrix, sometimes called a variance-covariance matrix. For the three ground
coordinates of one point, a covariance matrix is a 3 by 3 symmetrical matrix, with the matrix rows
and columns each corresponding to the three coordinates. For just the two horizontal ground
coordinates, a covariance matrix is a 2 by 2 symmetrical matrix, with the matrix rows and columns
each corresponding to the two horizontal coordinates. The matrix elements are the expected
average values of the product of the error in the row coordinate times the simultaneous error in the
column coordinate.

In a covariance matrix, the diagonal elements are the error variances of the corresponding ground
coordinates, or the squares of the standard deviations. The off-diagonal elements are the
covariances between the errors in the corresponding ground coordinates; these covariances will be
zero when the errors in different coordinates are not statistically correlated. The covariance matrix
is always symmetrical, meaning that the same element values appear on both sides of the diagonal
elements.

Covariance matrices can be used to record absolute and/or relative accuracies. A covariance matrix
for relative accuracy uses the 3 coordinates of one point for matrix rows and the 3 coordinates of
the second point for matrix columns. A complete covariance matrix for N specific points would
contain 3N rows and 3N columns.

A 3 by 3 covariance matrix provides six independent numbers, while horizontal CE plus vertical
LE are only two independent numbers. The information contained will be equivalent when:

1. The errors in the three ground coordinates are not statistically correlated.

2. The errors in the two horizontal coordinates have the same statistics.

When covariance matrices are computed and/or retrieved, the corresponding horizontal CE plus
vertical LE are often computed for presentation to data users who have no need for more complex
accuracy data. The horizontal CE plus vertical LE that correspond to a 3 by 3 covariance matrix can
be computed from that covariance matrix. Similarly, the horizontal CE that corresponds to a 2 by 2
covariance matrix can be computed from that covariance matrix. In such computations, the off-
diagonal matrix elements are usually neglected. The vertical LE with 0.6827 probability is the
square root of the vertical coordinate variance, if the vertical coordinate error has a normal
probability distribution. The horizontal CE is a somewhat complex function of the two horizontal
coordinate variances and of the covariance between these coordinates (when known).

2.8. Confidence Probability
A variety of different values are used for the confidence probability of a Linear Error or Circular
Error. Existing standardized probability values include 0.5, 0.6827, 0.9, and 0.95. A probability of
0.6827 corresponds to the standard deviation of a quantity with a normal distribution. (Note that a
Circular Error does not have a normal distribution, but may be derived from independent normal
distributions of the errors in each horizontal position coordinate.) The 0.9 probability was used in
the old United States National Map Accuracy Standards. The 0.95 probability is used in the Draft



The OpenGIS™ Abstract Specification Page 5

Volume 9: Quality (99-109r1.doc)

Geospatial Positioning Accuracy Standards. The 0.5 probability is now used in certain military
applications.

2.9. Normal Error Distribution
The actual statistical distribution of errors is usually unknown, but is usually assumed to be a
normal distribution for a Linear Error. A normal distribution is often assumed because:

1. Experimental tests of Linear Error distributions have often produced results that approximate a
normal distribution.

2. A total error that is a combination of many statistically independent component errors tends
toward a normal distribution, for any statistical distributions of the component errors.

When a normal distribution is assumed, the corresponding error values for different confidence
probabilities are directly related. That is, the error distance for a different probability can be
obtained by multiplying the distance by a constant. Different constant multipliers are needed for
Linear Errors and for Circular Errors (assuming the individual coordinates have normal error
distributions). Standard tables of multipliers exist for converting error distance values among 0.5,
0.6827, 0.9, and 0.95 probabilities. Table 1 is one such table. The constant multipliers listed for CE
assume certain properties of the covariance matrix for the two horizontal coordinates. Specifically,
these multipliers assume that the horizontal coordinates have the same variances with zero
covariance between the two coordinates.

Error Type Error Confidence Probability
0.3935 0.50 0.6827 0.90 0.95

LE 0.6745 1.0000 1.6449 1.9600
CE 1.0000 1.1774 2.1460 2.4485

Table 2-1. Error Magnitude as Function of Probabilities.

When a normal distribution applies, all elements of a covariance matrix have a 0.6827 probability.

2.10. Status of the Abstract Specification for Quality
This version of the Abstract Specification for Quality is an introduction to the technology, and

focuses more on positional and geometry accuracy than on the broader topic of Quality.

Section 3 of this document is an “essential model,” not an “abstract model” in the sense of Cook
and Daniels [6].

This abstract specification for Quality has been waiting for mature “Quality” standards to emerge
from the ISO TC/211 activity.

It should be expected that as the Committee Draft of the TC/211 circulates, it and the comments it
generates will become the foundation for the next version of this document.

2.11. References for Section 2
[1] OpenGIS™ Abstract Specification, OpenGIS™ Project Documents 99-100 through 99-116,

available through www as <http://www.opengis.org/techno/specs.htm>.

[2] An Essential Specification for Coverages and Images, OpenGIS Project Document 96-024R2

[3] Universal Image Geometry Model, OpenGIS Project Document 97-003R1

[4] Draft Geospatial Positioning accuracy Standards, Federal Geographic Data Committee, December
1996

[5] United States National Map Accuracy Standards, June 17, 1947

[6] Cook, Steve, and John Daniels, Designing Objects Systems: Object-Oriented Modeling with
Syntropy, Prentice Hall, New York, 1994, xx + 389 pp.



The OpenGIS™ Abstract Specification Page 6

Volume 9: Quality (99-109r1.doc)

3. Abstract Model for Quality

3.1. Accuracy and Other Metadata
The ability to record accuracy and other metadata for each Feature object is provided by allowing a
relationship between a Feature object and a Metadata Set object. A Metadata Set object contains
one or more Metadata Entity objects. Each Metadata Entity object contains multiple attributes that
record values of metadata elements.

Many sub-classes of the Metadata Entity class can be defined. Nine of these Metadata Entity sub-
classes contain accuracy data for Feature, Geometry, and Coverage objects, with tentative sub-class
names:

1. Absolute Position Accuracy

2. Relative Position Accuracy

3. Monoscopic Image Absolute Accuracy

4. Monoscopic Image Relative Accuracy

5. Stereoscopic Images Absolute Accuracy

6. Stereoscopic Images Relative Accuracy

7. Metric Property Accuracy

8. Non-metric Property Accuracy

9. Intersection Threshold

All nine listed sub-classes have multiple attributes for recording values of accuracy metadata
elements. The Absolute and Relative Position Accuracy sub-classes are appropriate for Geometry
and some Coverage objects. The four Monoscopic and Stereoscopic Image Accuracy sub-classes
are for images and certain other Coverages. The last three listed sub-classes are for other Feature
properties.

Figure 3-1 is a class diagram showing the existing Feature and Feature Collection classes and these
new metadata classes with the relationships between them. This diagram does not include the other
previously defined classes to which the Feature and Feature Collection classes have relationships.
The following subsections discuss each class shown in this diagram.



The OpenGIS™ Abstract Specification Page 7

Volume 9: Quality (99-109r1.doc)

Abso l
Accur

Covari
Estimat

(existing
relationships) contains

describes

Feature Collection

*
Feature

(existing
relationships)

1

describes

* 0..1

contains

Metadata Set

Metadata Entity
(abstract)

Entity Type Name
*

Absolute Position
Accuracy

Covariance Matrix
Estimation Method

Relative Position
Accuracy

Bin Max. Distance
Bin Min. Distance
Covariance Matrix
Estimation Method

Metric Property
Accuracy

Property Name
Property Error
Probability
Estimation Method

1 *

Entity Type Name

1
1

1

contains1

*

Monoscopic Image
Absolute Accuracy

Compute Total Errors

Estimation Method
Horizontal Shear
Second Image List

Monoscopic Image
Relative Accuracy

Compute Total Errors

Bin Max. Distance
Bin Min. Distance
Estimation Method
Second Image List

Stereoscopic Images
Absolute Accuracy

Compute Total Errors

Estimation Method
Left Image List
Right Image List
Y Parallax
Horizontal Shear
Vertical Shear

1 *

Non-metric 
Property Accuracy

Property Name
Probability
Estimation Method

Stereoscopic Images
Relative Accuracy

Compute Total Errors

Bin Max. Distance
Bin Min. Distance
Estimation Method
Left Image List
Right Image List

1 *

* *

Intersection
Threshold

Threshold Distance
Intersection 
      Dimensions

0..1

Figure 3-1 Accuracy Metadata Classes and Relationships

3.1.1. Feature
A Feature object, previously defined in the OpenGIS Abstract Specification, has one new
relationship, to one Metadata Set object. This relationship is optional, included only when metadata
is recorded for that Feature object. This relationship is recorded by a mandatory property of a
Feature with the property name Metadata. The value of the metadata property is normally the ID of
the associated Metadata Set object. If this property value is null, there is no related Metadata Set
object, and all aspects of the Feature accuracy not associated with the Feature Collection(s) that
contains this Feature are assumed to be unknown or unspecified.

3.1.2. Feature Collection
A Feature Collection object, previously defined in the OpenGIS Abstract Specification, has one
new relationship to one Metadata Set object. This relationship is required, and is recorded by a
mandatory property of a Feature Collection with the property name Metadata. The value of the



The OpenGIS™ Abstract Specification Page 8

Volume 9: Quality (99-109r1.doc)

metadata property is the ID of the associated Metadata Set object. If this property value is null,
there is no related Metadata Set object, and all metadata are assumed to be unknown or unspecified
except for any metadata associated with the individual features contained in that collection or with
a higher level Feature Collection that contains this Feature Collection.

3.1.3. Metadata Set
A Metadata Set object contains, or is related to, a set of Metadata Entity objects. Each Metadata Set
object is related to one or more Feature objects, or to one Feature Collection object.

3.1.4. Metadata Entity
A Metadata Entity object contains a set of object attributes that store the values of a logical group
of metadata elements. Each metadata element is recorded as a name and value pair of object
attributes. Since the set of metadata elements depends on the metadata to be recorded, the Metadata
Entity class is abstract, and a concrete subclass is used for each different logical group of metadata
elements to be recorded.

The Metadata Entity class defines one attribute included in all subclasses, a string giving the name
of the specific Metadata Entity subclass. Each such name is expected to include a version
identification number. Each Metadata Entity object is included in (related to) one or more Metadata
Set objects.

Multiple subclasses of the Metadata Entity class are defined, with the specific attributes included in
each subclass being a logical group of metadata elements. The following Sections specify several
specific subclasses of the Metadata Entity class, defined for recording Geometry, Coverage, and
Feature property accuracies.

3.2. Position Accuracy Metadata Entities
The Absolute and Relative Position Accuracy sub-classes of Metadata Entities record position
accuracy data for Features with geometry or some coverage types.

3.2.1. Absolute Position Accuracy
An Absolute Position Accuracy object is a Metadata Entity object that records the absolute
accuracy of the Geometry or Coverage Generator object that is included in a Feature object. That is,
such an object records statistical measures of the position accuracy of the Geometry or Coverage
relative to the specified spatial reference system. For a simple Feature, these statistical measures
apply to all the points used in that Geometry. For a Coverage, these statistical measures apply to all
the points or other items included in that Coverage.

The additional attributes included in an Absolute Position Accuracy object shall be name and value
pairs that contain:

1. Covariance Matrix, a symmetrical square matrix of floating point numbers in meters squared
units. The matrix dimensions shall correspond to the number of coordinates recorded for a point in
a feature. Matrix elements shall be missing or have null values when an actual value for that
element is not known.

2. Estimation Method, a string (alphanumeric text). This defines the method used to estimate the
values in the Covariance Matrix.

The indirect inclusion of one Absolute Position Accuracy object in each Metadata Set object is
optional, but is strongly encouraged whenever the related Feature object includes a Geometry or a
relevant Coverage Generator function. If no Absolute Position Accuracy object is included, the
position accuracy of the Geometry shall be interpreted as unknown or unspecified. Inclusion of
more than one Absolute Position Accuracy object is allowed, using multiple objects to contain
absolute accuracy data with different error Estimation Methods.

3.2.2. Relative Position Accuracy
Each Relative Position Accuracy object is a Metadata Entity object that records relative position
accuracy data for the Geometry or Coverage Generator object that is included in a Feature object.
That is, such an object contains statistical measures of the position accuracy between two points in
the Geometry or Coverage. Each Relative Position Accuracy object contains accuracy data for one
distance bin between points. For a simple Feature, these statistical measures apply to all the points



The OpenGIS™ Abstract Specification Page 9

Volume 9: Quality (99-109r1.doc)

used in that Geometry. For a Coverage, these statistical measures apply to all the points or other
items included in that Coverage.

The additional attributes in a Relative Position Accuracy object shall be name and value pairs that
contain:

1. Bin Minimum Distance, one number in meter units

2. Bin Maximum Distance, one number in meter units

3. Covariance Matrix, a symmetrical square matrix of floating point numbers in meters squared
units. The matrix dimensions shall correspond to the number of coordinates recorded for a point in
a feature. Matrix elements shall be missing or have null values when an actual value for that
element is not known.

4. Estimation Method, a string (alphanumeric text). This defines the method used to estimate the
Covariance Matrix. This text should specify or imply the quality of the values provided in the
Covariance Matrix.

The indirect inclusion of one or more Relative Position Accuracy objects in each Metadata Set
object is optional but is encouraged whenever the related Feature object includes a Geometry or a
relevant Coverage Generator function. Inclusion of more than one Relative Position Accuracy
object is allowed and encouraged for different distance bins. Multiple Relative Position Accuracy
objects can also contain relative accuracy data with different error Estimation Methods.

If no Relative Position Accuracy object is included, the relative position accuracy shall be
interpreted as being 1.4 times the Absolute Position Accuracy specified values in the Covariance
Matrix. If no Absolute Position Accuracy object is included, the relative position accuracy of the
Geometry shall be interpreted as unknown or unspecified.

3.3. Image Position Accuracy Metadata Entities
The Absolute and Relative Position Accuracy objects defined above do not contain all the accuracy
data needed when the related Coverage Generator object is an image or certain other types of
Coverage. Therefore, four separate Metadata Entity subclasses are defined for images, that include
different attributes for recording this different position accuracy data. Since the appropriate
accuracy data is somewhat different for monoscopic and stereoscopic images, separate Metadata
Entity subclasses are defined for monoscopic and stereoscopic images.

For vector feature Geometries, the Covariance Matrix reflects the total error in the recorded
coordinates of each point. For images, the ground coordinates are computed (directly or indirectly)
from the image position coordinates (row and column) of an object. For images, Covariance Matrix
values are recorded reflecting the estimated errors in ground positions of points computed from
perfect image position coordinates. These ground positions are computed using the associated
image geometry model and image support data. These accuracies reflect the estimated errors in the
image support data and image geometry model, but not the errors of measuring points in the
images.

In the monoscopic and stereoscopic image subclasses of the Metadata Entity class, the Covariance
Matrix attribute (as defined in the Absolute and Relative Position Accuracy subclass) may exist but
is private. That is, this attribute is not directly accessible since it does not represent the total error in
a ground point extracted from the image(s). The total error includes a contribution due to the error
in measuring the desired point in the image(s).

Therefore, these image accuracy subclasses shall each have an interface operation (or function) to
compute and return the total position error estimates. These interface operations use inputs
specifying the estimated errors in measuring a point in the image(s). For monoscopic images, these
interface operations also use inputs specifying the estimated errors in the ground elevations used
with the image positions to determine the ground horizontal position. The ability to compute total
errors depends on certain (unspecified) private attributes of these objects, such as partial derivatives
of ground position errors produced by image position errors. The ground position errors produced
by image position measurement errors are expected to be combined with the other errors by
computing the Root Mean Square (RMS) of the error components.

The four Monoscopic and Stereoscopic Image Accuracy sub-classes of Metadata Entities record
absolute and relative position accuracy data for images and certain other Coverages.



The OpenGIS™ Abstract Specification Page 10

Volume 9: Quality (99-109r1.doc)

3.3.1. Monoscopic Image Absolute Accuracy
A Monoscopic Image Absolute Accuracy object is a Metadata Entity object that records absolute
position accuracy data for an image Coverage Generator function that is included in a Feature
object. One Monoscopic Image Absolute Accuracy object should be indirectly included in a
Metadata Set object whenever it is related to an image. The public attributes included in a
Monoscopic Image Absolute Accuracy object shall be name and value pairs that contain:

1. Estimation Method, a string (alphanumeric text). This defines the method used to estimate the
Horizontal Shear and the (non-visible) covariance matrices representing the image geometry model
and support data absolute errors.

2. Horizontal Shear, one floating point number in meter units. This shear is the root-mean-square
horizontal distance between ground positions derived from the related image and from any other
listed overlapping images.

3. Second Image List, a sequence of strings (alphanumeric text). Each string identifies one other
image to which this Horizontal Shear applies. That is, the specified Horizontal Shear is between a
point measured in the related image and a point measured in any of the listed other images that
overlaps the first image. The related image can also be included in this list, with no meaning.

A Monoscopic Image Absolute Accuracy object includes the interface operation Compute Total
Errors. This operation computes the total error and returns:

1. Ground Covariance Matrix, a symmetrical 3 by 3 matrix of floating point numbers in meters
squared units. This matrix estimates the total errors in the horizontal ground position computed.
Matrix elements shall be missing or have null values when an actual value for that element is not
known.

The Compute Total Errors operation inputs the parameters:

1. Image Position Covariance Matrix,  a symmetrical 2 by 2 matrix of floating point numbers in
pixel spacing squared units. The off-diagonal matrix elements shall be missing or have null values
when an actual value for that element is not known.

2. Elevation Variance, one floating point number in meters squared units. This is the estimated
error in the elevation or height value used with an image position to compute the corresponding
ground position

3. Image Position, two numbers in pixel spacing units. Inclusion of this input allows the accuracy to
depend on the actual image position.

4. Elevation, one number in meter units. Inclusion of this input allows the accuracy to depend on
the actual ground elevation.

3.3.2. Monoscopic Image Relative Accuracy
A Monoscopic Image Relative Accuracy object is a Metadata Entity object that records relative
position accuracy data for an image Coverage Generator function that is included in a Feature
object. The public attributes included in a Monoscopic Image Relative Accuracy object shall be
name and value pairs that contain:

1. Bin Minimum Distance, one number in meter units

2. Bin Maximum Distance, one number in meter units

3. Estimation Method, a string (alphanumeric text). This defines the method used to estimate the
(non-visible) covariance matrices representing the image geometry model and support data relative
errors.

4. Second Image List, a sequence of strings (alphanumeric text). Each string identifies one other
image to which this relative accuracy data applies. That is, the specified relative accuracy is
between a point measured in the related image and a point measured in any of the listed other
images. The related image can also be included in this list, meaning that the relative accuracy
applies between two points measured in that one image.



The OpenGIS™ Abstract Specification Page 11

Volume 9: Quality (99-109r1.doc)

A Monoscopic Image Relative Accuracy object includes the interface operation Compute Total
Errors. This operation computes the total error and returns:

1. Ground Covariance Matrix, a symmetrical 3 by 3 matrix of floating point numbers in meters
squared units. This matrix estimates the total errors between two horizontal ground positions
computed. Matrix elements shall be missing or have null values when an actual value for that
element is not known.

The Compute Total Errors operation inputs the parameters:

1. First Image Position Covariance Matrix, a symmetrical 2 by 2 matrix of floating point numbers
in pixel spacing squared units. The off-diagonal matrix elements shall be missing or have null
values when an actual value for that element is not known.

2. Second Image Position Covariance Matrix, a symmetrical 2 by 2 matrix of floating point
numbers in pixel spacing squared units. The off-diagonal matrix elements shall be missing or have
null values when an actual value for that element is not known.

3. First Elevation Variance, one floating point number in meters squared units. This is the estimated
error in the elevation value used with the first image position to compute the corresponding ground
position.

4. Second Elevation Variance, one floating point number in meters squared units. This is the
estimated error in the elevation value used with the second image position to compute the
corresponding ground position.

5. First Image Position, two numbers in pixel spacing units. Inclusion of this input allows the
accuracy to depend on the actual image position.

6. Second Image Position, two numbers in pixel spacing units. Inclusion of this input allows the
accuracy to depend on the actual image position.

7. First Elevation, one number in meter units. Inclusion of this input allows the accuracy to depend
on the actual ground elevation.

8. Second Elevation, one number in meter units. Inclusion of this input allows the accuracy to
depend on the actual ground elevation.

One or more Monoscopic Image Relative Accuracy objects should be indirectly included in a
Metadata Set object whenever it is related to an image Coverage. Inclusion of more than one
Monoscopic Image Relative Accuracy object is allowed and encouraged for different distance bins.
Furthermore, separate objects can be used for different groups of second images that have
significantly different relative accuracies. Multiple Relative Position Accuracy objects can also
contain relative accuracy data with different error Estimation Methods.

3.3.3. Stereoscopic Images Absolute Accuracy
A Stereoscopic Images Absolute Accuracy object is another Metadata Entity object that records
absolute position accuracy data for an image Coverage Generator function that is included in a
Feature object. The public attributes included in a Stereoscopic Images Absolute Accuracy object
shall be name and value pairs that contain:

1. Estimation Method, a string (alphanumeric text). This defines the method used to estimate the Y
Parallax, Horizontal Shear, Vertical Shear, and the (non-visible) covariance matrices representing
the image geometry model and support data absolute errors.

2. Left Image List, a sequence of strings (alphanumeric text). The first string identifies one image
of the stereoscopic pair to which this accuracy data applies, namely the image expected to be
manually viewed with the left eye. The first image can also be included later in this list, with no
meaning.

3. Right Image List, a sequence of strings (alphanumeric text). This sequence must contain the
same number of elements as the Left Image List. The first string identifies one image of the
stereoscopic pair to which this accuracy data applies, namely the image expected to be manually
viewed with the right eye. The first image can also be included later in this list, with no meaning.



The OpenGIS™ Abstract Specification Page 12

Volume 9: Quality (99-109r1.doc)

4. Y Parallax, one floating point number in meter units. This Y Parallax is the root-mean-square of
the minimum distance between ground coordinates derived from the first listed stereoscopic pair of
images.

5. Horizontal Shear, one floating point number in meter units. This shear is the root-mean-square
horizontal distance between ground coordinates derived from the first listed pair of images and
from any other listed stereoscopic pairs of images that overlap it.

6. Vertical Shear, one floating point number in meter units. This shear is the root-mean-square
vertical difference between ground coordinates derived from the first listed pair of images and from
any other listed stereoscopic pairs of images that overlap it.

A Stereoscopic Images Absolute Accuracy object includes the interface operation Compute Total
Errors. This operation computes the total error and returns:

1. Ground Covariance Matrix, a symmetrical 3 by 3 matrix of floating point numbers in meters
squared units. Matrix elements shall be missing or have null values when an actual value for that
element is not known.

The Compute Total Errors operation inputs the parameters:

1. Left Image Position Covariance Matrix, a symmetrical 2 by 2 matrix of floating point numbers in
pixel spacing squared units. The off-diagonal matrix elements shall be missing or have null values
when an actual value for that element is not known.

2. Right Image Position Covariance Matrix, a symmetrical 2 by 2 matrix of floating point numbers
in pixel spacing squared units. The off-diagonal matrix elements shall be missing or have null
values when an actual value for that element is not known.

3. Left Image Position, two numbers in pixel spacing units. Inclusion of this input allows the
accuracy to depend on the actual image position.

4. Right Image Position, two numbers in pixel spacing units. Inclusion of this input allows the
accuracy to depend on the actual image position.

One or more Stereoscopic Images Absolute Accuracy objects should be indirectly included in a
Metadata Set object whenever it is related to an image that is expected to be exploited
stereoscopically. Separate Stereoscopic Images Absolute Accuracy objects should be included for
each other image with which the Coverage image is expected to be exploited stereoscopically. One
object can be used for both images of one stereopair. However, one object cannot be used for
multiple stereopairs that have essentially the same absolute accuracy.

In addition to giving the ground position accuracy derivable from a stereoscopic pair of images,
each Stereoscopic Images Absolute Accuracy object specifies one pair of images that can be
exploited stereoscopically.

3.3.4. Stereoscopic Images Relative Accuracy
A Stereoscopic Images Relative Accuracy object is another Metadata Entity object that records
relative position accuracy data for an image Coverage Generator function that is included in a
Feature object. The public attributes included in a Stereoscopic Images Relative Accuracy object
shall be name and value pairs that contain:

1. Bin Minimum Distance, one number in meter units

2. Bin Maximum Distance, one number in meter units

3. Estimation Method, a string (alphanumeric text). This defines the method used to estimate the
(non-visible) covariance matrices representing the image geometry model and support data relative
errors.



The OpenGIS™ Abstract Specification Page 13

Volume 9: Quality (99-109r1.doc)

4. Left Image List, a sequence of strings (alphanumeric text). Each string identifies one image of a
stereoscopic pair to which this accuracy data applies, namely the image expected to be manually
viewed with the left eye. The available relative accuracy data is between a point measured in the
first listed image and a point measured in any of the other listed images. The first image can also be
included later in this list, meaning that the relative accuracy applies between two points measured
in the same left image.

5. Right Image List, a sequence of strings (alphanumeric text). This sequence must contain the
same number of elements as the Left Image List. Each string identifies one image of a stereoscopic
pair to which this accuracy data applies, namely the image expected to be manually viewed with
the right eye. The available relative accuracy data is between a point measured in the first listed
image and a point measured in any of the other listed images. The first image can also be included
later in this list, meaning that the relative accuracy applies between two points measured in the
same right image.

A Stereoscopic Images Relative Accuracy object includes the interface operation Compute Total
Errors. This operation computes the total error and returns:

1. Ground Covariance Matrix, a symmetrical 3 by 3 matrix of floating point numbers in meters
squared units. Matrix elements shall be missing or have null values when an actual value for that
element is not known.

The Compute Total Errors operation inputs the parameters:

1. First Left Image Position Covariance Matrix, a symmetrical 2 by 2 matrix of floating point
numbers in pixel spacing squared units. The off-diagonal matrix elements shall be missing or have
null values when an actual value for that element is not known.

2. First Right Image Position Covariance Matrix, a symmetrical 2 by 2 matrix of floating point
numbers in pixel spacing squared units. The off-diagonal matrix elements shall be missing or have
null values when an actual value for that element is not known.

3. Second Left Image Position Covariance Matrix, a symmetrical 2 by 2 matrix of floating point
numbers in pixel spacing squared units. The off-diagonal matrix elements shall be missing or have
null values when an actual value for that element is not known.

4. Second Right Image Position Covariance Matrix, a symmetrical 2 by 2 matrix of floating point
numbers in pixel spacing squared units. The off-diagonal matrix elements shall be missing or have
null values when an actual value for that element is not known.

5. First Left Image Position, two numbers in pixel spacing units. Inclusion of this input allows the
accuracy to depend on the actual image position.

6. First Right Image Position, two numbers in pixel spacing units. Inclusion of this input allows the
accuracy to depend on the actual image position.

7. Second Left Image Position, two numbers in pixel spacing units. Inclusion of this input allows
the accuracy to depend on the actual image position.

8. Second Right Image Position, two numbers in pixel spacing units. Inclusion of this input allows
the accuracy to depend on the actual image position.

One or more Stereoscopic Images Relative Accuracy objects should be indirectly included in a
Metadata Set object whenever it is related to an image that is expected to be exploited
stereoscopically. Inclusion of more than one Stereoscopic Images Relative Accuracy object is
allowed and encouraged for different distance bins. Multiple Relative Position Accuracy objects
can also contain relative accuracy data with different error Estimation Methods.

Separate Stereoscopic Images Relative Accuracy objects should be included for each other image
with which the Coverage image is expected to be exploited stereoscopically. Separate Stereoscopic
Images Relative Accuracy objects can be included for each of several groups of nearby stereopairs.
Separate objects should be used when different groups of stereopairs have significantly different
relative error estimates.



The OpenGIS™ Abstract Specification Page 14

Volume 9: Quality (99-109r1.doc)

3.4. Property Accuracy Metadata Entities
The Metric Property Accuracy, Non-metric Property Accuracy, and Intersection Threshold sub-
classes of the Metadata Entity subclass record accuracy data for other Feature properties.

3.4.1. Metric Property Accuracy
A Metric Property Accuracy object is a Metadata Entity object that records the accuracy of a
Feature property having a numerical value. Each Feature object should be indirectly related to a
Metadata Set object that contains one Metric Property Accuracy object for each Feature property
with a numerical value. If no Metric Property Accuracy object is included for a particular Feature
property, the accuracy of that property value shall be interpreted as unknown or unspecified.

The additional attributes in a Metric Property Accuracy object shall be name and value pairs that
contain:

1. Property Name, a string (alphanumeric text), identifying the specific property

2. Property Error, one floating point number in the same units as the property value

3. Probability, enumeration of the values 0.5, 0.6827, 0.9, and 0.95 probability

4. Estimation Method, a string (alphanumeric text). This defines the method used to estimate the
error in the property value.

3.4.2. Non-metric Property Accuracy
A Non-metric Property Accuracy object is a Metadata Entity object that records the accuracy of a
Feature property having an enumeration or string (text) value. Each Feature object should be
indirectly related to a Metadata Set object that contains one Non-metric Property Accuracy object
for each Feature property with an enumeration value. This Metadata Set object can also contain a
Non-metric Property Accuracy object for some Feature properties with string values. If no Non-
metric Property Accuracy object is included for a particular Feature property, the accuracy of that
property value shall be interpreted as unknown or unspecified.

A Non-metric Property Accuracy object should also be used for the feature type to which a Feature
object has been assigned, to record the probability that the assigned feature type is correct. If the
next-most-likely feature types were also recorded as Feature properties, those feature type
properties could also have their probabilities recorded. Similarly, if other next-most-likely property
values were recorded as separate Feature properties, those alternative property values could also
have their probabilities recorded. The result would be equivalent to recording a column (or row) of
the confusion matrix, either the entire column or the most significant elements of a column.

The additional attributes in a Non-metric Property Accuracy object shall be name and value pairs
that contain:

1. Property Name, a string (alphanumeric text), identifying the specific property

2. Probability, one floating point number giving the probability that the recorded (enumerated or
text) property value is correct

3. Estimation Method, a string (alphanumeric text). This defines the method used to estimate the
probability that the property value is correct.

3.4.3. Intersection Threshold
For some Geometry subtypes, the detection of any “incorrect” intersections between different parts
of that Geometry is required, to ensure that a Geometry to be recorded is valid. Computation is
required to detect an intersection between two line segments, or between a point and a line
segment. Even double precision floating point computations have limited accuracy, depending on
the equations implemented and the computation order. Furthermore, any subsequent transformation
between spatial reference systems will introduce small errors. These (small) errors can cause two
items to intersect or not intersect, when the opposite was true prior to the spatial reference system
transformation.

The original author (Arliss Whiteside) does not know how Feature intersection accuracy should be
handled in the OpenGIS® specifications. In certain previous work, he assumed that any two items
were intended to intersect if they came within a specified small threshold distance of each other.



The OpenGIS™ Abstract Specification Page 15

Volume 9: Quality (99-109r1.doc)

When this occurred, one or both items were automatically modified so that they used points with
exactly the same recorded coordinates.

An Intersection Threshold object is a Metadata Entity object that records the criteria used to check
if a Geometry intersected itself incorrectly. Such an object should be indirectly included in the
Metadata Set object that is related to a Feature with a Geometry that could intersect itself
incorrectly. The simple Geometry subclasses that could self-intersect include Curve and Surface.

The additional attributes in an Intersection Threshold object might be name and value pairs that
contain:

1. Threshold Distance, one floating point number in meter units

2. Intersection Dimensions, one short integer, specifying whether 2 or 3 dimensions were
considered in checking intersections

3.5. Discussion
The remainder of this Section provides a discussion of the topics introduced above.  Section 3.6
provides some interpretations, and Section 3.7 provides assumptions.  Section 3.8 lists Alternatives,
and Section 3.9 discusses error estimation methods. Section 3.10 discusses possible levels of
position accuracy.  Section 3.11 provides possible levels of property accuracy.  Section 3.12
provides detail on CE and LE computations.

3.6. Interpretations
Whenever a defined and applicable accuracy Metadata Entity object is not indirectly included in a
Metadata Set object, that accuracy shall be interpreted as unknown or unspecified. Any user
without additional information should assume that the unspecified accuracy is poor (should never
assume that the recorded value is perfect).

Most of the listed attributes of Metadata Set objects and Metadata Entity objects might be read
only. Certainly these attributes will be modified only during or immediately following object
creation, often before the new object is fully related to other objects.

This proposal allows recording different accuracies for different types of Feature objects. That is,
higher accuracies can be recorded for feature types consisting of “well defined points” as used in
the Draft Geospatial Positioning Accuracy Standards, and lower accuracies can be recorded for
feature types with not-so-well-defined points.

Many accuracy numbers are defined as being in meter units, since meters are an international
standard for distances. Since accuracies are recorded using floating point numbers, the use of meter
units does not imply any expected position accuracy. A position accuracy number can be many
meters or only a few millimeters.

Some of the accuracy attributes discussed here are discussed from a different perspective in
Sections 6.1.8, 6.2.5, and 6.2.6 of OpenGIS™ Project Document 97-003R1.

3.7. Assumptions
Key elements of this proposal assume that the errors in each ground coordinate have a normal
probability distribution. For example, the elements of a covariance matrix have 0.6827 probability
for a normal distribution, but not for most other probability distributions. The user of position
accuracy metadata normally should assume normal error distributions. If the data producer expects
some other probability distribution, the expected probability distribution should be recorded and
transferred. The expected probability distribution should be recorded as part of the Estimation
Method, or pointed to by the Estimation Method.

An image will often have multiple versions of its image support data, and perhaps of its image
geometry model, with later versions providing higher accuracy or better computation efficiency.
The same original image can thus have several different position accuracies. We assume that each
version of an image with its support data will be recorded as a different Coverage. We also assume
that each Image ID referred to in this proposal will include a support data version identification.
This Version ID could be in the same Image ID string, or it could be recorded in a separate object
attribute.



The OpenGIS™ Abstract Specification Page 16

Volume 9: Quality (99-109r1.doc)

Multiple versions of the same original image can also exist, which have geometries that differ in
simple ways. These multiple versions of one original image may exist simultaneously and/or at
different times. For example, multiple reduced resolution versions of an image are often created
and stored. Also, different patches or segments of an original image (or of a reduced resolution
image) can be retrieved at different times, by the same or different users. Rotated, scaled, and
simply-warped versions of an image can also be created and used.

Different versions of the same image which have somewhat different geometries could use separate
accuracy metadata. Alternately, only the accuracy metadata for the original complete, full
resolution image might be stored. This metadata would be used somewhat differently with each
image version, as needed to account for the image version geometry differences. For example, the
image positions and accuracies found in an image version would be transformed into the
corresponding values for the original image.

When a Feature object has a Geometry property defined by an OpenGIS® WKS, the position
accuracies of the WKS points (or vertices) are described by Absolute Position Accuracy and
perhaps Relative Position Accuracy objects. If the Geometry is a Curve or a Surface, its positions
are partly defined by interpolation between adjacent vertices. We assume that vertices are spaced
sufficiently close together so that the specified interpolation algorithm does not introduce
significant additional position error. Alternately, the Absolute and Relative Position Accuracy
objects should represent the accuracy of the complete Curve or a Surface object after interpolation
is applied.

One set of accuracy data is provided for an entire Geometry property defined by an Open GIS
WKS, although a Geometry can contain multiple points or vertices. Recording of one set of
accuracy data for a Geometry assumes that either:

1. The accuracies of all vertices are similar, and the typical accuracy is thus sufficient information.
By similar accuracy, we mean the same Horizontal CE and Vertical LE (with a certain probability)
within perhaps 30 percent.

2. The user has no practical way to use accuracy data that varies between the vertices of one
Geometry object.

If different parts of a Geometry object have significantly different accuracies and if this is useful
information, then each part should be recorded as a separate Feature and Geometry object. Each
such Feature (and Geometry) object could then have a different recorded accuracy. A composite
Feature can then be recorded that consists of all these smaller Features. Since this composite
Feature does not directly include a Geometry, it would not be directly related to position accuracy
data.

Although not fully described, we assume that a Feature Collection object can also be related to a
Metadata Set object, for recording the metadata that applies to all features in the collection.

3.8. Alternatives
Many alternatives are possible to the objects and relationships defined here, including:

1. Record Horizontal Circular Error and Vertical Linear Error instead of, or in addition to,
recording a covariance matrix of ground coordinate errors.

2. Record a 3N by 3N covariance matrix to record both the absolute and relative accuracies of N
specific points.

3. Define simpler versions of the Image Absolute and Relative Accuracy subclasses, that contain
less data. For example, Vertical Shear, Horizontal Shear, Y Parallax, and/or Image Lists might be
eliminated. These simpler versions could be alternatives, or might replace the more complex
versions. (These classes would be easier to understand, but could not record all the accuracy data
that NIMA now keeps.)

4. Record relative accuracy data as a specified continuous function of the distance between two
points (instead of using a discrete number of distance bins).

5. Require the recorded accuracy to be constant over an image, instead of allowing the accuracy to
be a function of the image position.



The OpenGIS™ Abstract Specification Page 17

Volume 9: Quality (99-109r1.doc)

6. Record a confusion probability matrix for selected enumerated-value properties of a Feature,
including the assignment of the Feature to a specific feature type. This matrix could be recorded
instead of, or in addition to, recording Non-metric Property Accuracy objects as defined herein.

7. Record the Metadata Set Object ID in a special attribute of a Feature, separate from the Feature
property pairs of attributes.

8. Not record each metadata element in a Metadata Entity object with a corresponding name
attribute (like properties are recorded in a Feature object). (This would be somewhat more efficient,
but might be less flexible.)

9. Record all Probability attributes as floating point numbers, instead of enumerations.

10. Relate a Geometry or Coverage Generator object directly to a Metadata Set object that (only)
records position accuracy metadata.

3.9. Error Estimation Methods
A variety of methods could be used to estimate position and property accuracies. Probably a
standard set of methods should be defined, with corresponding standard (text string) values defined
for the values placed in the Error Estimation Method attributes. A standard set of Error Estimation
Methods might include: (numbers are not significant)

1. Professional Estimate, by an appropriate trained, registered, or certified professional, such as a
surveyor, civil engineer, or photogrammetrist.

2. Computed Estimate, by combining estimates of all the significant error components. The error
component estimates will normally be produced by a variety of methods.

3. Compared to Similar Quality Data, produced somewhat independently. If the similar data is
produced by the same or similar methods, that data is likely to include some highly correlated error
components. In this case, just using the value differences will produce overly optimistic error
estimates. The error estimates produced from value differences should thus be increased to include
the estimated errors not reflected in the difference data.

4. Tested Similar Quality Data, by comparing similar data produced by the same methods to
significantly higher quality data.

5. Tested Sample Actual Data, by comparing a sample of the actual values in this Feature
Collection to significantly higher quality data.

For items 4 and 5 listed above, the significantly higher quality data, or “ground truth” data, must
have estimated standard deviation (or CE and LE) errors less than 1/3 of the resulting estimated
errors, preferably less than 1/10. Furthermore, the error estimates produced should include the
estimated errors in the reference data used. (As previously stated, the error estimates produced for
vector Features also should include an allowance for the interpolation error between recorded
vertices.)

In any case, the Error Estimation Method attribute should include an explicit or implicit reference
to information (other metadata) describing in more detail the specific methods used.

3.10. Possible Levels of Position Accuracy
Many alternate forms of position accuracy data could be recorded, ranging from very simple to
very complex. This proposal specifies one way in which to record most of the position accuracy
data currently used for mapping images by the National Imagery and Mapping Agency (NIMA).
This NIMA image accuracy data is fairly extensive and complex, and may not be needed by many
users of a GIS. Therefore, this proposal allows recording of only a subset of the defined accuracy
data. For example, inclusion of Relative Position Accuracy objects is optional. When included,
separate Relative Position Accuracy objects could be provided for only one or two distance bins, or
could be provided for many distance bins.

On the other hand, recording of more extensive or different accuracy data could be supported by
future extensions of the structures defined in this proposal. For example, the ability to record
covariance matrices across multiple points could be allowed, in addition to or in place of recording
Absolute Covariance Matrices for single points and Relative Covariance Matrices between two



The OpenGIS™ Abstract Specification Page 18

Volume 9: Quality (99-109r1.doc)

points. This is likely to lead to the definition of multiple “levels” for recording position accuracy
data, with the accuracy data proposed herein considered to be one or more of a larger set of
accuracy data levels.

Some possible levels of position accuracy data are: (numbers are not significant)

1. No Position Accuracy - No position accuracy data is included.

2. Uncorrelated Absolute Position Accuracy - Absolute position accuracy covariance matrices are
included without off-diagonal covariance matrix elements (and without relative accuracy data).

3. Correlated Absolute Position Accuracy - Absolute position accuracy covariance matrices are
included with all off-diagonal covariance matrix elements (but not relative accuracy data).

4. Basic Relative Position Accuracy - In addition to absolute position accuracy data, relative
position accuracy covariance matrices are included for one or two distance bins.

5. Extended Relative Position Accuracy - In addition to absolute position accuracy data, relative
position accuracy covariance matrices are included for three or more distance bins.

6. Variable Position Accuracy - Absolute and relative position accuracy data is provided in a form
that allows the accuracy to vary with the point position in one Feature, image, or stereopair.

7. Multiple Point Covariance Matrices - In addition to absolute and relative position accuracy
varying with the point position, the absolute and relative position accuracy data is accessible in the
form of 3N by 3N covariance matrices, where N refers to the number of points.

For images recorded using level items 2 through 5 above, the absolute and relative position error
estimates are constant over each Feature, image, or stereopair.

A Feature containing multiple parts, or a Feature Collection, should be considered to have the
lowest position accuracy level that is provided for any included component.

3.11. Possible Levels of Property Accuracy
For levels of property accuracy, the usually-multiple properties of one Feature need to be
considered. However, the accuracy of certain properties is usually perfect or not relevant, such as
properties that point to metadata or to the specification of the spatial reference system. All
properties to which property accuracy applies should have the property accuracy specified in the
Feature metadata. If a Feature Collection contains multiple types of Features, all relevant properties
of all Feature types should have the Property Accuracy specified in the Feature (or Feature
Collection) metadata.

Some possible levels of property accuracy data are: (numbers are not significant)

1. No Property Accuracy - No property accuracy data is included, or the provided property
accuracy data is not complete.

2. Metric Property Accuracy - All relevant numerical or metric properties have associated Metric
Property Accuracy objects.

3. Enumerated Property Accuracy - In addition to all relevant metric property accuracies, all
relevant enumerated and text string properties have associated Non-metric Property Accuracy
objects.

4. Enumerated Property Confusion Matrices - In addition to all relevant metric property accuracies,
each relevant enumerated property has associated objects recording confusion matrices or rows.

3.12. CE and LE Computations
Whenever a GIS data user desires simpler accuracy data than a covariance matrix, standardized
software can be used to compute horizontal CE and vertical LE from the proper covariance matrix.
Such transformations of accuracy data might be performed by a service class or object that provides
operations to:

1. Convert Ground Covariances to CE and LE. The operation inputs would include the 3 by 3
Ground Position Covariance Matrix (in meters squared units) and the desired confidence



The OpenGIS™ Abstract Specification Page 19

Volume 9: Quality (99-109r1.doc)

Probability. The operation outputs would include the Horizontal CE and Vertical LE (in meter
units).

2. Convert Horizontal Covariances to CE. The inputs would include the 2 by 2 Horizontal Ground
Position Covariance Matrix (in meters squared units) and the desired confidence Probability. The
outputs would include the Horizontal CE (in meter units).

3. Convert Image Covariances to CE. The inputs would include the 2 by 2 Image Position
Covariance Matrix (in pixel spacing squared units) and the desired confidence Probability. The
outputs would include the Image Position CE (in pixel spacing units). This operation might be
combined with item 2.

4. Convert Variance to LE. The inputs would include the Variance (in squared units) and the
desired confidence Probability. The outputs would include the Linear Error (LE) (in un-squared
units).

5. Convert CE and LE to Ground Covariances. The inputs would include the Horizontal CE and
Vertical LE (in meter units) and the corresponding confidence Probability. The outputs would
include the 3 by 3 Ground Position Covariance Matrix (in meters squared units), with all the off-
diagonal matrix elements being either omitted or having null values. In addition, the two horizontal
coordinate variances would be the same in the output covariance matrix.

6. Convert CE to Horizontal Covariances. The inputs would include the Horizontal CE (in meter
units) and the corresponding confidence Probability. The outputs would include the 2 by 2
Horizontal Ground Position Covariance Matrix (in meters squared units), with the off-diagonal
matrix elements being either omitted or having null values. In addition, the two horizontal
coordinate variances would be the same in the output covariance matrix.

7. Convert CE to Image Covariances. The inputs would include the Image Position CE (in pixel
spacing units) and the corresponding confidence Probability. The outputs would include the 2 by 2
Image Position Covariance Matrix (in pixel spacing squared units), with the off-diagonal matrix
elements being either omitted or having null values. In addition, the two image coordinate
variances would be the same in the output covariance matrix. This operation might be combined
with item 6.

8. Convert LE to Variance. The inputs would include the Linear Error (LE) and the corresponding
confidence Probability. The outputs would include the Variance (in the same units squared).

All the operations listed above might assume a normal probability distribution for the errors in each
position coordinate. Alternately, these operations might use an additional input specifying the
probability distribution to be used.

3.13. References for Section 3
[1] OpenGIS™ Abstract Specification, OpenGIS™ Project Documents 99-100 through 99-116,

available through www as <http://www.opengis.org/techno/specs.htm>.



The OpenGIS™ Abstract Specification Page 20

Volume 9: Quality (99-109r1.doc)

4. Future Work
The classes, attributes, and interface functions proposed here probably need to be further refined.
We hope that recommended and possible refinements will result from discussion of this proposal
by an OpenGIS™ group. For example, specific objects or operations might be specified for
converting between different error probabilities (assuming normal distributions).

This work clearly needs to be extended to define a way to describe the accuracy of transformations
between coordinate systems, datums, etc. Furthermore, mechanisms are needed to ensure that
accuracy descriptions are updated when transformations are used (preferably updated behind the
scenes, without human interaction).

We do not yet understand how available coordinate transformations would be recorded and
exercised. Attaching accuracy data to transformations clearly depends on how the transformations
are recorded. Perhaps the accuracy of transformations could be recorded using Metadata Set objects
containing Metadata Entity objects similar to the "Absolute Position Accuracy" and "Relative
Position Accuracy" objects.

Ideally, the same object operations that perform transformations would also automatically generate
combined accuracy data. If this is not practical, operations of the appropriate Metadata Entity
subtypes could generate combined accuracy data. This might be done using operations similar to
"Compute Total Errors," that have input parameters for the ground position errors before the
corresponding transformation is performed.



The OpenGIS™ Abstract Specification Page 21

Volume 9: Quality (99-109r1.doc)

5. Appendix A. Well Known Structures
The WKS needed to carry the Accuracy Information specified in this Topic are tbd.


